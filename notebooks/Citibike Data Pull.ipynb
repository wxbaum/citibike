{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69fb9511",
   "metadata": {},
   "source": [
    "# Citibike Data Pull\n",
    "\n",
    "I've seperated the data pull from the analyses as it takes around a \n",
    "minute per month of data, and I've written a lot of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2995d65",
   "metadata": {},
   "source": [
    "## Notebook Setup & Data Pull\n",
    "\n",
    "First we need to get installations and imports out of the way, as well as load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6c5923",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:05:43.465895Z",
     "start_time": "2023-12-05T23:05:43.459894Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f76fa",
   "metadata": {},
   "source": [
    "## Data Retrieval, Formatting, and Memory Reduction\n",
    "The data is available in S3 here: https://s3.amazonaws.com/tripdata/index.html\n",
    "\n",
    "Each file in the S3 bucket is 1 month of data with the filename formatted like YYYYMM-citibike-tripdata.csv.zip.\n",
    "\n",
    "While Pandas can sometimes handle reading zipped CSV files directly, we get a bunch of unicode errors if we attempt it here. As such, we'll explicitly unzip the files then read them into dataframes. \n",
    "\n",
    "Critically, each month of data is hundreds of MB, if not well over a GB. To pull just a year of data will start burning through memory rather quickly. I want this to work out of the box on most computers so we'll put particular emphasis on reducing memory consumption in this section. \n",
    "\n",
    "Below this markdown cell there's a global setting that skips the data retrieval. If data retrieval has been run before, the code will skip straight to reading the files from disk.\n",
    "\n",
    "Firstly, we'll write a function that retrieves a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d5377f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:12:54.205402Z",
     "start_time": "2023-12-05T23:12:54.199932Z"
    }
   },
   "outputs": [],
   "source": [
    "FETCH_RAW_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1061d",
   "metadata": {},
   "source": [
    "### Data Download \n",
    "All functions for downloading data from S3, reducing memory, downloading each month of data, and writing data to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbdc6f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:07.882191Z",
     "start_time": "2023-12-05T23:03:07.864534Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_tripdata_file(yyyymm:str, sample_only=False):\n",
    "    \"\"\"\n",
    "    Downloads a single CSV file from https://s3.amazonaws.com/tripdata.\n",
    "    \n",
    "    Args:\n",
    "        yyyymm: Year and month for the target file.\n",
    "        sample_only: If set to True, this will return only five rows of data, the aim being to retrieve the file structure\n",
    "          while not committing the data in its entirity to a dataframe object. Note that the full raw file itself will be\n",
    "          downloaded within the scope of this function, but discarded on function exit. \n",
    "        \n",
    "    Returns:\n",
    "        Pandas dataframe containing Citi Bike trip data.\n",
    "    \"\"\"\n",
    "    # URL of the file\n",
    "    csvzip_url = f'https://s3.amazonaws.com/tripdata/{yyyymm}-citibike-tripdata.csv.zip'\n",
    "    zip_url = f'https://s3.amazonaws.com/tripdata/{yyyymm}-citibike-tripdata.zip'\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    # If .csv.zip doesn't work, remove the .csv and try again\n",
    "    response = requests.get(csvzip_url)\n",
    "    if not response.ok:\n",
    "        response = requests.get(zip_url)\n",
    "\n",
    "    # Ensure the response content is a zip file\n",
    "    if response.ok:\n",
    "        # Read the content of the response as a zip file\n",
    "        with BytesIO(response.content) as f:\n",
    "            with ZipFile(f) as zipfile:\n",
    "                # Extract the names of files in the zip file\n",
    "                csv_files = [name for name in zipfile.namelist() if name.endswith('.csv')]\n",
    "                if csv_files:\n",
    "                    # Read the first CSV file into a Pandas DataFrame\n",
    "                    if sample_only is False:\n",
    "                        df = pd.read_csv(\n",
    "                            zipfile.open(csv_files[0]),\n",
    "                            low_memory=False\n",
    "                        )\n",
    "                        \n",
    "                    elif sample_only is True:\n",
    "                        df = pd.read_csv(\n",
    "                            zipfile.open(csv_files[0]),\n",
    "                            low_memory=False,\n",
    "                            nrows=5\n",
    "                        )\n",
    "                        \n",
    "                    else:\n",
    "                        raise ValueError('sample_only must be a boolean value')\n",
    "\n",
    "                else:\n",
    "                    print(\"No CSV files found in the zip archive.\")\n",
    "                    \n",
    "                return df\n",
    "            \n",
    "    else:\n",
    "        print(\"Failed to retrieve the file. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae9e045",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.061869Z",
     "start_time": "2023-12-05T23:03:07.886552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35B687A564CE21E7</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-04-21 16:36:53</td>\n",
       "      <td>2024-04-21 16:45:38</td>\n",
       "      <td>Ave A &amp; E 14 St</td>\n",
       "      <td>5779.11</td>\n",
       "      <td>Broadway &amp; E 21 St</td>\n",
       "      <td>6098.10</td>\n",
       "      <td>40.730311</td>\n",
       "      <td>-73.980472</td>\n",
       "      <td>40.739888</td>\n",
       "      <td>-73.989586</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BE10BC14E481C1F7</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-04-04 17:22:45</td>\n",
       "      <td>2024-04-04 17:52:56</td>\n",
       "      <td>Broadway &amp; W 36 St</td>\n",
       "      <td>6441.01</td>\n",
       "      <td>E 102 St &amp; 1 Ave</td>\n",
       "      <td>7407.13</td>\n",
       "      <td>40.750977</td>\n",
       "      <td>-73.987654</td>\n",
       "      <td>40.786995</td>\n",
       "      <td>-73.941648</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25B32779E0C39E2D</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-04-20 23:14:41</td>\n",
       "      <td>2024-04-20 23:23:02</td>\n",
       "      <td>Lewis Ave &amp; Greene Ave</td>\n",
       "      <td>4543.07</td>\n",
       "      <td>McKibbin St &amp; Bogart St</td>\n",
       "      <td>5059.02</td>\n",
       "      <td>40.689848</td>\n",
       "      <td>-73.936447</td>\n",
       "      <td>40.706237</td>\n",
       "      <td>-73.933871</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BD7E92E1B80CFCAA</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-04-22 21:49:44</td>\n",
       "      <td>2024-04-22 22:03:54</td>\n",
       "      <td>E 2 St &amp; Ave C</td>\n",
       "      <td>5476.03</td>\n",
       "      <td>Graham Ave &amp; Grand St</td>\n",
       "      <td>5178.06</td>\n",
       "      <td>40.720955</td>\n",
       "      <td>-73.981159</td>\n",
       "      <td>40.711863</td>\n",
       "      <td>-73.944024</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D88ACE119443C57E</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-04-12 18:11:15</td>\n",
       "      <td>2024-04-12 18:28:42</td>\n",
       "      <td>Lewis Ave &amp; Greene Ave</td>\n",
       "      <td>4543.07</td>\n",
       "      <td>Graham Ave &amp; Grand St</td>\n",
       "      <td>5178.06</td>\n",
       "      <td>40.690181</td>\n",
       "      <td>-73.936524</td>\n",
       "      <td>40.711863</td>\n",
       "      <td>-73.944024</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           started_at             ended_at  \\\n",
       "0  35B687A564CE21E7   classic_bike  2024-04-21 16:36:53  2024-04-21 16:45:38   \n",
       "1  BE10BC14E481C1F7   classic_bike  2024-04-04 17:22:45  2024-04-04 17:52:56   \n",
       "2  25B32779E0C39E2D  electric_bike  2024-04-20 23:14:41  2024-04-20 23:23:02   \n",
       "3  BD7E92E1B80CFCAA  electric_bike  2024-04-22 21:49:44  2024-04-22 22:03:54   \n",
       "4  D88ACE119443C57E  electric_bike  2024-04-12 18:11:15  2024-04-12 18:28:42   \n",
       "\n",
       "       start_station_name  start_station_id         end_station_name  \\\n",
       "0         Ave A & E 14 St           5779.11       Broadway & E 21 St   \n",
       "1      Broadway & W 36 St           6441.01         E 102 St & 1 Ave   \n",
       "2  Lewis Ave & Greene Ave           4543.07  McKibbin St & Bogart St   \n",
       "3          E 2 St & Ave C           5476.03    Graham Ave & Grand St   \n",
       "4  Lewis Ave & Greene Ave           4543.07    Graham Ave & Grand St   \n",
       "\n",
       "   end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \n",
       "0         6098.10  40.730311 -73.980472  40.739888 -73.989586        member  \n",
       "1         7407.13  40.750977 -73.987654  40.786995 -73.941648        member  \n",
       "2         5059.02  40.689848 -73.936447  40.706237 -73.933871        member  \n",
       "3         5178.06  40.720955 -73.981159  40.711863 -73.944024        member  \n",
       "4         5178.06  40.690181 -73.936524  40.711863 -73.944024        member  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sample from the month before last so we can see it\n",
    "month_before_last = (datetime.now() - timedelta(days=60)).strftime('%Y%m')\n",
    "data_sample = download_tripdata_file(month_before_last, True)\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46034ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.089680Z",
     "start_time": "2023-12-05T23:03:22.078035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ride_id             5 non-null      object \n",
      " 1   rideable_type       5 non-null      object \n",
      " 2   started_at          5 non-null      object \n",
      " 3   ended_at            5 non-null      object \n",
      " 4   start_station_name  5 non-null      object \n",
      " 5   start_station_id    5 non-null      float64\n",
      " 6   end_station_name    5 non-null      object \n",
      " 7   end_station_id      5 non-null      float64\n",
      " 8   start_lat           5 non-null      float64\n",
      " 9   start_lng           5 non-null      float64\n",
      " 10  end_lat             5 non-null      float64\n",
      " 11  end_lng             5 non-null      float64\n",
      " 12  member_casual       5 non-null      object \n",
      "dtypes: float64(6), object(7)\n",
      "memory usage: 648.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "data_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce201017",
   "metadata": {},
   "source": [
    "From the dataframe sample and dataframe info above, we see the layout of our file and the data types we're dealing with. \n",
    "\n",
    "So - memory consumption. Data types are going to be important here as apart from dropping columns outright, that's the primary way we'll be able get the data size down. \n",
    "\n",
    "What can we drop? There doesn't appear to be superfluous data, however the station names, ids, and locations are going to be duplicative across the data. When we retrieve each file, we can create a dataframe of unique identified stations, then leave only the IDs in the data, removing 2 object columns and 4 float64 columns in the process. We'll then have ride data and a seperate dataset of stations that we can merge to ad-hoc.   \n",
    "\n",
    "We can do similar with rideable type and member_casual, identifying unique rideables and unique membership types.\n",
    "\n",
    "Furthermore, we've got two columns with datetime strings. We can calculate the ride time in seconds using the difference of the two datetimes, then remove the end time, replacing a memory hogging object dtype column with an integer column. \n",
    "\n",
    "That's a few different things to do, so we'll break it into functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a746213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.134134Z",
     "start_time": "2023-12-05T23:03:22.119899Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_unique_stations(df):\n",
    "    \"\"\"\n",
    "    Identifies unique Citi Bike stations using start stations and end stations. Memory conserved by casting lat longs as\n",
    "      float16.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe of ride data as pulled from S3. \n",
    "        \n",
    "    Returns:\n",
    "        Dataframe containing all uniquely identified stations IDs, names, and locations. \n",
    "    \"\"\"    \n",
    "    # Get unique start stations\n",
    "    start_station_cols = ['start_station_id', 'start_station_name', 'start_lat', 'start_lng']\n",
    "    unique_start_stations = df.loc[:, start_station_cols].drop_duplicates()\n",
    "    unique_start_stations.columns = ['station_id', 'station_name', 'lat', 'lng']\n",
    "    \n",
    "    # Get unique end stations\n",
    "    end_station_cols = ['end_station_id', 'end_station_name', 'end_lat', 'end_lng']\n",
    "    unique_end_stations = df.loc[:, end_station_cols].drop_duplicates()\n",
    "    unique_end_stations.columns = ['station_id', 'station_name', 'lat', 'lng']\n",
    "    \n",
    "    # Concatenate unique start and end stations then drop duplicates and reset index\n",
    "    unique_stations = pd.concat(\n",
    "        [unique_start_stations, unique_end_stations]\n",
    "    ).drop_duplicates('station_id').reset_index(drop=True)\n",
    "    \n",
    "    # Now we'll drop the current ID and reset index again to create integer unique IDs for each station\n",
    "    unique_stations = unique_stations.reset_index(names=['int_station_id'])   \n",
    "    \n",
    "    # ***Memory usage***\n",
    "    # Cast lats and lons as float32 as we've got more decimal places than the 4 float16 would allow\n",
    "    for col in ['lat', 'lng']:\n",
    "        unique_stations[col] = unique_stations[col].astype('float32')\n",
    "    # We have to use int16, not int8 for station_id as there are over 127 Citi Bike stations\n",
    "    unique_stations['int_station_id'] = unique_stations['int_station_id'].astype('int16')\n",
    "    \n",
    "    return unique_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ba3db",
   "metadata": {},
   "source": [
    "To demo what this looks like, here's the unique station dataframe from the data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569212f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.186373Z",
     "start_time": "2023-12-05T23:03:22.145894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_station_id</th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5779.11</td>\n",
       "      <td>Ave A &amp; E 14 St</td>\n",
       "      <td>40.730312</td>\n",
       "      <td>-73.980469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6441.01</td>\n",
       "      <td>Broadway &amp; W 36 St</td>\n",
       "      <td>40.750977</td>\n",
       "      <td>-73.987656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4543.07</td>\n",
       "      <td>Lewis Ave &amp; Greene Ave</td>\n",
       "      <td>40.689846</td>\n",
       "      <td>-73.936447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5476.03</td>\n",
       "      <td>E 2 St &amp; Ave C</td>\n",
       "      <td>40.720955</td>\n",
       "      <td>-73.981155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6098.10</td>\n",
       "      <td>Broadway &amp; E 21 St</td>\n",
       "      <td>40.739887</td>\n",
       "      <td>-73.989586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7407.13</td>\n",
       "      <td>E 102 St &amp; 1 Ave</td>\n",
       "      <td>40.786995</td>\n",
       "      <td>-73.941650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5059.02</td>\n",
       "      <td>McKibbin St &amp; Bogart St</td>\n",
       "      <td>40.706238</td>\n",
       "      <td>-73.933868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5178.06</td>\n",
       "      <td>Graham Ave &amp; Grand St</td>\n",
       "      <td>40.711864</td>\n",
       "      <td>-73.944023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_station_id  station_id             station_name        lat        lng\n",
       "0               0     5779.11          Ave A & E 14 St  40.730312 -73.980469\n",
       "1               1     6441.01       Broadway & W 36 St  40.750977 -73.987656\n",
       "2               2     4543.07   Lewis Ave & Greene Ave  40.689846 -73.936447\n",
       "3               3     5476.03           E 2 St & Ave C  40.720955 -73.981155\n",
       "4               4     6098.10       Broadway & E 21 St  40.739887 -73.989586\n",
       "5               5     7407.13         E 102 St & 1 Ave  40.786995 -73.941650\n",
       "6               6     5059.02  McKibbin St & Bogart St  40.706238 -73.933868\n",
       "7               7     5178.06    Graham Ave & Grand St  40.711864 -73.944023"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_unique_stations(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcb2b9",
   "metadata": {},
   "source": [
    "Now onto unique rideable types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "782569d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.212573Z",
     "start_time": "2023-12-05T23:03:22.204006Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_unique_rideables(rideables):\n",
    "    \"\"\"\n",
    "    Identifies unique rideable types, e.g. classic_bike. \n",
    "    \n",
    "    Args:\n",
    "        rideables: Series containing the rideable_type data.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe containing unique rideables with a newly generated identifier\n",
    "    \"\"\"    \n",
    "    # Get unique rideables\n",
    "    unique_rideables = pd.DataFrame(rideables.drop_duplicates(keep='first'))\n",
    "    \n",
    "    # Now we'll use reset index to add a unique identifier\n",
    "    # Reset twice as we dropped a bunch of dupes so the index is currently not sequential\n",
    "    # Resetting with drop then resetting without createts a sequential set of IDs\n",
    "    unique_rideables = unique_rideables.reset_index(drop=True).reset_index(names=['rideable_id'])\n",
    "    \n",
    "    # ***Memory usage***\n",
    "    # Cast lats and lons as float16\n",
    "    unique_rideables['rideable_id'] = unique_rideables['rideable_id'].astype('int8')\n",
    "    \n",
    "    return unique_rideables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33e4cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.257417Z",
     "start_time": "2023-12-05T23:03:22.224328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rideable_id</th>\n",
       "      <th>rideable_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>classic_bike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>electric_bike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rideable_id  rideable_type\n",
       "0            0   classic_bike\n",
       "1            1  electric_bike"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_unique_rideables(data_sample['rideable_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0533def1",
   "metadata": {},
   "source": [
    "We can copy much the same code to do memberships as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d20b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.278367Z",
     "start_time": "2023-12-05T23:03:22.268089Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_unique_memberships(memberships):\n",
    "    \"\"\"\n",
    "    Identifies unique membership types, e.g. member vs casual. \n",
    "    \n",
    "    Args:\n",
    "        memberships: Series containing the membership_type data.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe containing unique memberships with a newly generated identifier\n",
    "    \"\"\"    \n",
    "    # Get unique rideables\n",
    "    unique_memberships = pd.DataFrame(memberships.drop_duplicates(keep='first'))\n",
    "    \n",
    "    # Now we'll use reset index to add a unique identifier\n",
    "    # Reset twice as we dropped a bunch of dupes so the index is currently not sequential\n",
    "    # Resetting with drop then resetting without createts a sequential set of IDs\n",
    "    unique_memberships = unique_memberships.reset_index(drop=True).reset_index(names=['membership_id'])\n",
    "    \n",
    "    # Rename member_casual\n",
    "    unique_memberships = unique_memberships.rename(columns={'member_casual': 'membership_type'})\n",
    "    \n",
    "    # ***Memory usage***\n",
    "    # Cast lats and lons as float16\n",
    "    unique_memberships['membership_id'] = unique_memberships['membership_id'].astype('int8')\n",
    "    \n",
    "    return unique_memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "977afe66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.337434Z",
     "start_time": "2023-12-05T23:03:22.309902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>membership_id</th>\n",
       "      <th>membership_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   membership_id membership_type\n",
       "0              0          member"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_unique_memberships(data_sample['member_casual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a0cdc",
   "metadata": {},
   "source": [
    "Now we'll address creating the ride time column from the started_at and ended_at columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cfde957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.356436Z",
     "start_time": "2023-12-05T23:03:22.345165Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_trip_duration_col(df):\n",
    "    \"\"\"\n",
    "    Calculates a series of trip durations in seconds, then downcasts data before returning. Memory conserved by casting \n",
    "      trip duration as int16. \n",
    "    \n",
    "    Args:\n",
    "        df: started_at and ended_at columns from dataframe. \n",
    "        \n",
    "    Returns:\n",
    "        Pandas series of trip durations.\n",
    "    \"\"\"\n",
    "    trip_duration = (pd.to_datetime(df['ended_at']) - pd.to_datetime(df['started_at'])).dt.seconds\n",
    "    \n",
    "    # ***Memory Usage***\n",
    "    # We choose int16 as it allows precision up to 32767, meaning in practicality, up to 8 hours of ride time\n",
    "    # This is a reasonable ride time (accuracy) cutoff, as only up to 45 minutes are included in the \"base price\"\n",
    "    trip_duration = trip_duration.astype('int16')\n",
    "    \n",
    "    return trip_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63692be8",
   "metadata": {},
   "source": [
    "We've completed our data formatting and ID generation processes, so now we need to actually build out the dataset. We'll be pulling multiple months of data, applying the formatting functions to each month as it's pulled. \n",
    "\n",
    "Below are some helper functions to reduce redundancy and make our final data build functions cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f65602fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.374034Z",
     "start_time": "2023-12-05T23:03:22.362859Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_on_columns(df, ref_df, left_on, right_on, new_col_name):\n",
    "    \"\"\"\n",
    "    Helper function to merge df with ref_df on specified columns and rename the new column.\n",
    "\n",
    "    Args:\n",
    "        df: The main DataFrame.\n",
    "        ref_df: The reference DataFrame to merge.\n",
    "        left_on: The column name in the main DataFrame to merge on.\n",
    "        right_on: The column name in the reference DataFrame to merge on.\n",
    "        new_col_name: The new column name after merging.\n",
    "\n",
    "    Returns:\n",
    "        Updated DataFrame after merge and column renaming.\n",
    "    \"\"\"\n",
    "    return (df.merge(ref_df[['station_id', 'int_station_id']], left_on=left_on, right_on=right_on, how='left')\n",
    "              .drop([right_on, left_on], axis=1)\n",
    "              .rename(columns={'int_station_id': new_col_name}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73616f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.394458Z",
     "start_time": "2023-12-05T23:03:22.385903Z"
    }
   },
   "outputs": [],
   "source": [
    "def concatenate_dataframes(dfs, drop_col=None):\n",
    "    \"\"\"\n",
    "    Concatenates a list of dataframes, drops duplicates, and resets the index.\n",
    "\n",
    "    Args:\n",
    "        dfs: List of dataframes to concatenate.\n",
    "        drop_col: Specific column to drop duplicates by, if needed.\n",
    "\n",
    "    Returns:\n",
    "        Concatenated dataframe.\n",
    "    \"\"\"\n",
    "    if drop_col is None:\n",
    "        return pd.concat(dfs).drop_duplicates().reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.concat(dfs).drop_duplicates(drop_col).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3a212f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.420957Z",
     "start_time": "2023-12-05T23:03:22.406092Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_and_report_memory_usage(initial_memory_values, dataframes):\n",
    "    \"\"\"\n",
    "    Calculates and reports the memory usage of the data processing.\n",
    "\n",
    "    Args:\n",
    "        initial_memory_values: List of memory usage values for each month's raw data.\n",
    "        dataframes: List of dataframes to include in the final memory usage calculation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Calculate initial and ending memory usage in MB\n",
    "    initial_memory_usage = sum(initial_memory_values) / 1024 / 1000\n",
    "    total_ending_memory_usage = sum(df.memory_usage(index=True, deep=True).sum() for df in dataframes) / 1024 / 1000\n",
    "    memory_saved = initial_memory_usage - total_ending_memory_usage\n",
    "\n",
    "    # Print memory usage information\n",
    "    print(f'Initial memory usage: {round(initial_memory_usage, 2)} MB')\n",
    "    print(f'Ending memory usage: {round(total_ending_memory_usage, 2)} MB')\n",
    "    print(f'Memory saved: {round(memory_saved, 2)} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83537f",
   "metadata": {},
   "source": [
    "Now we'll create a function that downloads a month of data and applies all the formatting work to that month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4a26600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.443952Z",
     "start_time": "2023-12-05T23:03:22.428888Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_formatted_month_rides(yyyymm):\n",
    "    \"\"\"\n",
    "    Downloads trip data file and conducts formatting. Formatting consists of creating a trip duration column.\n",
    "      \n",
    "    Args:\n",
    "        yyyymm: Year and month of target file.\n",
    "        \n",
    "    Returns:\n",
    "        Formatted dataframe of ride data, with multiple memory conservation steps applied. \n",
    "    \"\"\"\n",
    "    # Pull raw data\n",
    "    month_data = download_tripdata_file(yyyymm)\n",
    "    initial_memory_usage = month_data.memory_usage(index=True, deep=True).sum()\n",
    "\n",
    "    # Get unique stations, rideables, and memberships\n",
    "    unique_month_stations = identify_unique_stations(month_data)\n",
    "    unique_rideables = identify_unique_rideables(month_data['rideable_type'])\n",
    "    unique_memberships = identify_unique_memberships(month_data['member_casual'])\n",
    "\n",
    "    # Merge and format data\n",
    "    month_data = merge_on_columns(month_data, unique_month_stations, 'start_station_id', 'station_id', 'start_station_id')\n",
    "    month_data = merge_on_columns(month_data, unique_month_stations, 'end_station_id', 'station_id', 'end_station_id')\n",
    "    month_data = month_data.merge(unique_rideables, on='rideable_type', how='left')\n",
    "    month_data = month_data.merge(unique_memberships, left_on='member_casual', right_on='membership_type', how='left')\n",
    "\n",
    "    # Add trip duration\n",
    "    month_data['trip_duration'] = create_trip_duration_col(month_data[['started_at', 'ended_at']])\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = [\n",
    "        'ended_at', \n",
    "        'rideable_type', \n",
    "        'member_casual',\n",
    "        'membership_type',\n",
    "        'start_lat', \n",
    "        'end_lat', \n",
    "        'start_lng', \n",
    "        'end_lng', \n",
    "        'start_station_name', \n",
    "        'end_station_name'\n",
    "    ]\n",
    "    month_data = month_data.drop(drop_cols, axis=1)\n",
    "    \n",
    "    return month_data, unique_month_stations, unique_rideables, unique_memberships, initial_memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b0736",
   "metadata": {},
   "source": [
    "Finally, we use multiple iterations of create_formatted_month_rides to build a multi-month dataset with dramatically reduced memory consumption vs the raw files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535f95b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:05:40.365540Z",
     "start_time": "2023-12-05T23:05:40.343356Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_month_data(month, data_directory):\n",
    "    \"\"\"\n",
    "    Process and save data for a single month to a parquet file.\n",
    "\n",
    "    Args:\n",
    "        month: The month to process in yyyymm format.\n",
    "        data_directory: The directory where the parquet file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - The path to the saved parquet file.\n",
    "            - Unique stations DataFrame.\n",
    "            - Unique rideables DataFrame.\n",
    "            - Unique memberships DataFrame.\n",
    "            - Memory usage of the processed month data.\n",
    "    \"\"\"\n",
    "    month_data, unique_stations, unique_rideables, unique_memberships, memory_usage = create_formatted_month_rides(month)\n",
    "    file_path = f'{data_directory}/{month}_data.parquet'\n",
    "    month_data.to_parquet(file_path)\n",
    "\n",
    "    return file_path, unique_stations, unique_rideables, unique_memberships, memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43d05189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.642314Z",
     "start_time": "2023-12-05T23:03:22.609403Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_data(months_to_pull):\n",
    "    \"\"\"\n",
    "    Pulls requested trip data for each month in parallel, writes it to \n",
    "    parquet files in a parallel 'data' directory, and disposes of it from \n",
    "    memory. Memory conserved by dropping Citi Bike unique ID and by not \n",
    "    storing month data in memory. Unique data for stations, rideables, \n",
    "    and memberships are still aggregated in memory. The unique data \n",
    "    tables function as dimension tables are are written to a data \n",
    "    directory as such. \n",
    "\n",
    "    Args:\n",
    "        months_to_pull: List of yyyymm format months to create data with.\n",
    "\n",
    "    Returns: \n",
    "        A tuple containing:\n",
    "            - A list of file paths to the parquet files containing the ride data.\n",
    "            - DataFrames of all unique stations, rideables, and memberships.\n",
    "    \"\"\"\n",
    "    data_directory = '../data'  # Adjust this path according to your directory structure\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=len(months_to_pull)) as executor:\n",
    "        results = executor.map(process_month_data, months_to_pull, [data_directory] * len(months_to_pull))\n",
    "\n",
    "    # Unpack results\n",
    "    data_file_paths, all_unique_stations, all_unique_rideables, all_unique_memberships, month_file_memory_values = zip(*results)\n",
    "\n",
    "    # Concatenate and process unique dataframes\n",
    "    all_unique_stations = concatenate_dataframes(all_unique_stations, 'station_name')\n",
    "    all_unique_rideables = concatenate_dataframes(all_unique_rideables, 'rideable_type')\n",
    "    all_unique_memberships = concatenate_dataframes(all_unique_memberships, 'membership_type')\n",
    "\n",
    "    # Write uniques tables to data directory for use as fact tables\n",
    "    for df, filename in zip([all_unique_stations, all_unique_rideables, all_unique_memberships], ['stations', 'rideable_types', 'membership_types']):\n",
    "        output_filename = f'{filename}.parquet'\n",
    "        output_filepath = os.path.join(data_directory, output_filename)\n",
    "        df.to_parquet(output_filepath)\n",
    "\n",
    "    # Calculate and report memory usage\n",
    "    calculate_and_report_memory_usage(\n",
    "        month_file_memory_values, \n",
    "        [all_unique_stations, all_unique_rideables, all_unique_memberships]\n",
    "    )\n",
    "\n",
    "    return data_file_paths, all_unique_stations, all_unique_rideables, all_unique_memberships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bbfda",
   "metadata": {},
   "source": [
    "## Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60234358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable is at the top of the notebook also, but including here for convienience\n",
    "FETCH_RAW_DATA = True\n",
    "\n",
    "# SET MONTHS TO PULL\n",
    "months_to_pull = ['202403', '202404', '202405']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0673e67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:05:40.314640Z",
     "start_time": "2023-12-05T23:03:22.653496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 1503.45 MB\n",
      "Ending memory usage: 0.33 MB\n",
      "Memory saved: 1503.13 MB\n"
     ]
    }
   ],
   "source": [
    "if FETCH_RAW_DATA is True:\n",
    "    ride_data_paths, unique_stations, unique_rideables, unique_memberships = get_all_data(\n",
    "        months_to_pull\n",
    "    )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a98cf",
   "metadata": {},
   "source": [
    "Let's also output our unique lookup tables for reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "532c407a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.499080Z",
     "start_time": "2023-12-05T23:03:22.473531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_station_id</th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6283.05</td>\n",
       "      <td>48 St &amp; Skillman Ave</td>\n",
       "      <td>40.746155</td>\n",
       "      <td>-73.916191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5105.01</td>\n",
       "      <td>Liberty St &amp; Broadway</td>\n",
       "      <td>40.708858</td>\n",
       "      <td>-74.010231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6809.07</td>\n",
       "      <td>W 56 St &amp; 6 Ave</td>\n",
       "      <td>40.763405</td>\n",
       "      <td>-73.977226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_station_id station_id           station_name        lat        lng\n",
       "0               0    6283.05   48 St & Skillman Ave  40.746155 -73.916191\n",
       "1               1    5105.01  Liberty St & Broadway  40.708858 -74.010231\n",
       "2               2    6809.07        W 56 St & 6 Ave  40.763405 -73.977226"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_stations.head(3) # sample as this has a few hundred stations in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dfec875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.529030Z",
     "start_time": "2023-12-05T23:03:22.508834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rideable_id</th>\n",
       "      <th>rideable_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>electric_bike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>classic_bike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rideable_id  rideable_type\n",
       "0            0  electric_bike\n",
       "1            1   classic_bike"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_rideables.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "315b7e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.561180Z",
     "start_time": "2023-12-05T23:03:22.538879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>membership_id</th>\n",
       "      <th>membership_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   membership_id membership_type\n",
       "0              0          member\n",
       "1              1          casual"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_memberships.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b37a2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16b2e0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:27:19.590143Z",
     "start_time": "2023-12-05T23:27:09.189613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rides = pd.DataFrame()\n",
    "for path in ride_data_paths:\n",
    "    rides = pd.concat([rides, pd.read_parquet(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcb932ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:27:46.460817Z",
     "start_time": "2023-12-05T23:27:46.450400Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2291763 entries, 0 to 999999\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Dtype \n",
      "---  ------            ----- \n",
      " 0   ride_id           object\n",
      " 1   started_at        object\n",
      " 2   start_station_id  int16 \n",
      " 3   end_station_id    int16 \n",
      " 4   rideable_id       int8  \n",
      " 5   membership_id     int8  \n",
      " 6   trip_duration     int16 \n",
      "dtypes: int16(3), int8(2), object(2)\n",
      "memory usage: 69.9+ MB\n"
     ]
    }
   ],
   "source": [
    "rides.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabe01c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citibike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
