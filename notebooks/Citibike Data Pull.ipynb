{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69fb9511",
   "metadata": {},
   "source": [
    "# Citibike Data Pull\n",
    "\n",
    "I've seperated the data pull from the analyses as it takes around a \n",
    "minute per month of data, and I've written a lot of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2995d65",
   "metadata": {},
   "source": [
    "## Notebook Setup & Data Pull\n",
    "\n",
    "First we need to get installations and imports out of the way, as well as load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6c5923",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:05:43.465895Z",
     "start_time": "2023-12-05T23:05:43.459894Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f76fa",
   "metadata": {},
   "source": [
    "## Data Retrieval, Formatting, and Memory Reduction\n",
    "The data is available in S3 here: https://s3.amazonaws.com/tripdata/index.html\n",
    "\n",
    "Each file in the S3 bucket is 1 month of data with the filename formatted like YYYYMM-citibike-tripdata.csv.zip.\n",
    "\n",
    "While Pandas can sometimes handle reading zipped CSV files directly, we get a bunch of unicode errors if we attempt it here. As such, we'll explicitly unzip the files then read them into dataframes. \n",
    "\n",
    "Critically, each month of data is hundreds of MB, if not well over a GB. To pull just a year of data will start burning through memory rather quickly. I want this to work out of the box on most computers so we'll put particular emphasis on reducing memory consumption in this section. \n",
    "\n",
    "Below this markdown cell there's a global setting that skips the data retrieval. If data retrieval has been run before, the code will skip straight to reading the files from disk.\n",
    "\n",
    "Firstly, we'll write a function that retrieves a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d5377f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:12:54.205402Z",
     "start_time": "2023-12-05T23:12:54.199932Z"
    }
   },
   "outputs": [],
   "source": [
    "FETCH_RAW_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1061d",
   "metadata": {},
   "source": [
    "### Data Download \n",
    "All functions for downloading data from S3, reducing memory, downloading each month of data, and writing data to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbdc6f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:07.882191Z",
     "start_time": "2023-12-05T23:03:07.864534Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_tripdata_file(yyyymm:str, sample_only=False):\n",
    "    \"\"\"\n",
    "    Downloads a single CSV file from https://s3.amazonaws.com/tripdata.\n",
    "    \n",
    "    Args:\n",
    "        yyyymm: Year and month for the target file.\n",
    "        sample_only: If set to True, this will return only five rows of data, the aim being to retrieve the file structure\n",
    "          while not committing the data in its entirity to a dataframe object. Note that the full raw file itself will be\n",
    "          downloaded within the scope of this function, but discarded on function exit. \n",
    "        \n",
    "    Returns:\n",
    "        Pandas dataframe containing Citi Bike trip data.\n",
    "    \"\"\"\n",
    "    # URL of the file\n",
    "    csvzip_url = f'https://s3.amazonaws.com/tripdata/{yyyymm}-citibike-tripdata.csv.zip'\n",
    "    zip_url = f'https://s3.amazonaws.com/tripdata/{yyyymm}-citibike-tripdata.zip'\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    # If .csv.zip doesn't work, remove the .csv and try again\n",
    "    response = requests.get(csvzip_url)\n",
    "    if not response.ok:\n",
    "        response = requests.get(zip_url)\n",
    "\n",
    "    # Ensure the response content is a zip file\n",
    "    if response.ok:\n",
    "        # Read the content of the response as a zip file\n",
    "        with BytesIO(response.content) as f:\n",
    "            with ZipFile(f) as zipfile:\n",
    "                # Extract the names of files in the zip file\n",
    "                csv_files = [name for name in zipfile.namelist() if name.endswith('.csv')]\n",
    "                if csv_files:\n",
    "                    # Read the first CSV file into a Pandas DataFrame\n",
    "                    if sample_only is False:\n",
    "                        csv_datasets = []\n",
    "                        for csv_file in csv_files:\n",
    "                            df = pd.read_csv(\n",
    "                                zipfile.open(csv_file),\n",
    "                                low_memory=False\n",
    "                            )\n",
    "                            csv_datasets.append(df)\n",
    "                        df = pd.concat(csv_datasets)\n",
    "                        \n",
    "                    elif sample_only is True:\n",
    "                        df = pd.read_csv(\n",
    "                            zipfile.open(csv_files[0]),\n",
    "                            low_memory=False,\n",
    "                            nrows=5\n",
    "                        )\n",
    "                        \n",
    "                    else:\n",
    "                        raise ValueError('sample_only must be a boolean value')\n",
    "\n",
    "                else:\n",
    "                    print(\"No CSV files found in the zip archive.\")\n",
    "                    \n",
    "                return df\n",
    "            \n",
    "    else:\n",
    "        print(\"Failed to retrieve the file. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae9e045",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.061869Z",
     "start_time": "2023-12-05T23:03:07.886552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C943DA3BBC04DA57</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-06-19 19:24:11.778</td>\n",
       "      <td>2024-06-19 19:35:37.667</td>\n",
       "      <td>E 81 St &amp; York Ave</td>\n",
       "      <td>7084.12</td>\n",
       "      <td>E 84 St &amp; Park Ave</td>\n",
       "      <td>7243.04</td>\n",
       "      <td>40.772838</td>\n",
       "      <td>-73.949892</td>\n",
       "      <td>40.778627</td>\n",
       "      <td>-73.957721</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35F6F3D1D27DEF6D</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-06-20 17:01:54.322</td>\n",
       "      <td>2024-06-20 17:14:34.198</td>\n",
       "      <td>E 81 St &amp; York Ave</td>\n",
       "      <td>7084.12</td>\n",
       "      <td>Central Park West &amp; W 72 St</td>\n",
       "      <td>7141.07</td>\n",
       "      <td>40.772940</td>\n",
       "      <td>-73.949694</td>\n",
       "      <td>40.775794</td>\n",
       "      <td>-73.976206</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9B8071426046B632</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-06-25 15:39:31.800</td>\n",
       "      <td>2024-06-25 15:44:36.811</td>\n",
       "      <td>Broadway &amp; W 58 St</td>\n",
       "      <td>6948.10</td>\n",
       "      <td>Central Park West &amp; W 72 St</td>\n",
       "      <td>7141.07</td>\n",
       "      <td>40.766953</td>\n",
       "      <td>-73.981693</td>\n",
       "      <td>40.775794</td>\n",
       "      <td>-73.976206</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>995E66CC32088A47</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-06-26 15:29:28.377</td>\n",
       "      <td>2024-06-26 15:34:19.452</td>\n",
       "      <td>Broadway &amp; W 58 St</td>\n",
       "      <td>6948.10</td>\n",
       "      <td>Central Park West &amp; W 72 St</td>\n",
       "      <td>7141.07</td>\n",
       "      <td>40.766713</td>\n",
       "      <td>-73.981900</td>\n",
       "      <td>40.775794</td>\n",
       "      <td>-73.976206</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1D16A382B788B03E</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-06-21 19:42:04.451</td>\n",
       "      <td>2024-06-21 19:51:11.660</td>\n",
       "      <td>Banker St &amp; Meserole Ave</td>\n",
       "      <td>5633.04</td>\n",
       "      <td>Metropolitan Ave &amp; Bedford Ave</td>\n",
       "      <td>5308.04</td>\n",
       "      <td>40.726267</td>\n",
       "      <td>-73.956254</td>\n",
       "      <td>40.715348</td>\n",
       "      <td>-73.960241</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type               started_at  \\\n",
       "0  C943DA3BBC04DA57   classic_bike  2024-06-19 19:24:11.778   \n",
       "1  35F6F3D1D27DEF6D  electric_bike  2024-06-20 17:01:54.322   \n",
       "2  9B8071426046B632   classic_bike  2024-06-25 15:39:31.800   \n",
       "3  995E66CC32088A47  electric_bike  2024-06-26 15:29:28.377   \n",
       "4  1D16A382B788B03E  electric_bike  2024-06-21 19:42:04.451   \n",
       "\n",
       "                  ended_at        start_station_name  start_station_id  \\\n",
       "0  2024-06-19 19:35:37.667        E 81 St & York Ave           7084.12   \n",
       "1  2024-06-20 17:14:34.198        E 81 St & York Ave           7084.12   \n",
       "2  2024-06-25 15:44:36.811        Broadway & W 58 St           6948.10   \n",
       "3  2024-06-26 15:34:19.452        Broadway & W 58 St           6948.10   \n",
       "4  2024-06-21 19:51:11.660  Banker St & Meserole Ave           5633.04   \n",
       "\n",
       "                 end_station_name  end_station_id  start_lat  start_lng  \\\n",
       "0              E 84 St & Park Ave         7243.04  40.772838 -73.949892   \n",
       "1     Central Park West & W 72 St         7141.07  40.772940 -73.949694   \n",
       "2     Central Park West & W 72 St         7141.07  40.766953 -73.981693   \n",
       "3     Central Park West & W 72 St         7141.07  40.766713 -73.981900   \n",
       "4  Metropolitan Ave & Bedford Ave         5308.04  40.726267 -73.956254   \n",
       "\n",
       "     end_lat    end_lng member_casual  \n",
       "0  40.778627 -73.957721        member  \n",
       "1  40.775794 -73.976206        member  \n",
       "2  40.775794 -73.976206        member  \n",
       "3  40.775794 -73.976206        member  \n",
       "4  40.715348 -73.960241        casual  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sample from the month before last so we can see it\n",
    "month_before_last = (datetime.now() - timedelta(days=60)).strftime('%Y%m')\n",
    "data_sample = download_tripdata_file(month_before_last, True)\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46034ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.089680Z",
     "start_time": "2023-12-05T23:03:22.078035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ride_id             5 non-null      object \n",
      " 1   rideable_type       5 non-null      object \n",
      " 2   started_at          5 non-null      object \n",
      " 3   ended_at            5 non-null      object \n",
      " 4   start_station_name  5 non-null      object \n",
      " 5   start_station_id    5 non-null      float64\n",
      " 6   end_station_name    5 non-null      object \n",
      " 7   end_station_id      5 non-null      float64\n",
      " 8   start_lat           5 non-null      float64\n",
      " 9   start_lng           5 non-null      float64\n",
      " 10  end_lat             5 non-null      float64\n",
      " 11  end_lng             5 non-null      float64\n",
      " 12  member_casual       5 non-null      object \n",
      "dtypes: float64(6), object(7)\n",
      "memory usage: 648.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "data_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce201017",
   "metadata": {},
   "source": [
    "From the dataframe sample and dataframe info above, we see the layout of our file and the data types we're dealing with. \n",
    "\n",
    "So - memory consumption. Data types are going to be important here as apart from dropping columns outright, that's the primary way we'll be able get the data size down. \n",
    "\n",
    "What can we drop? There doesn't appear to be superfluous data, however the station names, ids, and locations are going to be duplicative across the data. When we retrieve each file, we can create a dataframe of unique identified stations, then leave only the IDs in the data, removing 2 object columns and 4 float64 columns in the process. We'll then have ride data and a seperate dataset of stations that we can merge to ad-hoc.   \n",
    "\n",
    "We can do similar with rideable type and member_casual, identifying unique rideables and unique membership types.\n",
    "\n",
    "Furthermore, we've got two columns with datetime strings. We can calculate the ride time in seconds using the difference of the two datetimes, then remove the end time, replacing a memory hogging object dtype column with an integer column. \n",
    "\n",
    "That's a few different things to do, so we'll break it into functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a746213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.134134Z",
     "start_time": "2023-12-05T23:03:22.119899Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_unique_stations(df):\n",
    "    \"\"\"\n",
    "    Identifies unique Citi Bike stations using start stations and end stations. Memory conserved by casting lat longs as\n",
    "      float16.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe of ride data as pulled from S3. \n",
    "        \n",
    "    Returns:\n",
    "        Dataframe containing all uniquely identified stations IDs, names, and locations. \n",
    "    \"\"\"    \n",
    "    # Get unique start stations\n",
    "    start_station_cols = ['start_station_id', 'start_station_name', 'start_lat', 'start_lng']\n",
    "    unique_start_stations = df.loc[:, start_station_cols].drop_duplicates()\n",
    "    unique_start_stations.columns = ['station_id', 'station_name', 'lat', 'lng']\n",
    "    \n",
    "    # Get unique end stations\n",
    "    end_station_cols = ['end_station_id', 'end_station_name', 'end_lat', 'end_lng']\n",
    "    unique_end_stations = df.loc[:, end_station_cols].drop_duplicates()\n",
    "    unique_end_stations.columns = ['station_id', 'station_name', 'lat', 'lng']\n",
    "    \n",
    "    # Concatenate unique start and end stations then drop duplicates and reset index\n",
    "    unique_stations = pd.concat(\n",
    "        [unique_start_stations, unique_end_stations]\n",
    "    ).drop_duplicates('station_id').reset_index(drop=True)\n",
    "    \n",
    "    # Now we'll drop the current ID and reset index again to create integer unique IDs for each station\n",
    "    unique_stations = unique_stations.reset_index(names=['int_station_id'])   \n",
    "    \n",
    "    # ***Memory usage***\n",
    "    # Cast lats and lons as float32 as we've got more decimal places than the 4 float16 would allow\n",
    "    for col in ['lat', 'lng']:\n",
    "        unique_stations[col] = unique_stations[col].astype('float32')\n",
    "    # We have to use int16, not int8 for station_id as there are over 127 Citi Bike stations\n",
    "    unique_stations['int_station_id'] = unique_stations['int_station_id'].astype('int16')\n",
    "    \n",
    "    return unique_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ba3db",
   "metadata": {},
   "source": [
    "To demo what this looks like, here's the unique station dataframe from the data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569212f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.186373Z",
     "start_time": "2023-12-05T23:03:22.145894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_station_id</th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7084.12</td>\n",
       "      <td>E 81 St &amp; York Ave</td>\n",
       "      <td>40.772839</td>\n",
       "      <td>-73.949890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6948.10</td>\n",
       "      <td>Broadway &amp; W 58 St</td>\n",
       "      <td>40.766953</td>\n",
       "      <td>-73.981697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5633.04</td>\n",
       "      <td>Banker St &amp; Meserole Ave</td>\n",
       "      <td>40.726269</td>\n",
       "      <td>-73.956253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7243.04</td>\n",
       "      <td>E 84 St &amp; Park Ave</td>\n",
       "      <td>40.778625</td>\n",
       "      <td>-73.957718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7141.07</td>\n",
       "      <td>Central Park West &amp; W 72 St</td>\n",
       "      <td>40.775795</td>\n",
       "      <td>-73.976204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5308.04</td>\n",
       "      <td>Metropolitan Ave &amp; Bedford Ave</td>\n",
       "      <td>40.715347</td>\n",
       "      <td>-73.960243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_station_id  station_id                    station_name        lat  \\\n",
       "0               0     7084.12              E 81 St & York Ave  40.772839   \n",
       "1               1     6948.10              Broadway & W 58 St  40.766953   \n",
       "2               2     5633.04        Banker St & Meserole Ave  40.726269   \n",
       "3               3     7243.04              E 84 St & Park Ave  40.778625   \n",
       "4               4     7141.07     Central Park West & W 72 St  40.775795   \n",
       "5               5     5308.04  Metropolitan Ave & Bedford Ave  40.715347   \n",
       "\n",
       "         lng  \n",
       "0 -73.949890  \n",
       "1 -73.981697  \n",
       "2 -73.956253  \n",
       "3 -73.957718  \n",
       "4 -73.976204  \n",
       "5 -73.960243  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_unique_stations(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcb2b9",
   "metadata": {},
   "source": [
    "Now onto unique rideable types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "782569d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.212573Z",
     "start_time": "2023-12-05T23:03:22.204006Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_unique_rideables(rideables):\n",
    "    \"\"\"\n",
    "    Identifies unique rideable types, e.g. classic_bike. \n",
    "    \n",
    "    Args:\n",
    "        rideables: Series containing the rideable_type data.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe containing unique rideables with a newly generated identifier\n",
    "    \"\"\"    \n",
    "    # Get unique rideables\n",
    "    unique_rideables = pd.DataFrame(rideables.drop_duplicates(keep='first'))\n",
    "    \n",
    "    # Now we'll use reset index to add a unique identifier\n",
    "    # Reset twice as we dropped a bunch of dupes so the index is currently not sequential\n",
    "    # Resetting with drop then resetting without createts a sequential set of IDs\n",
    "    unique_rideables = unique_rideables.reset_index(drop=True).reset_index(names=['rideable_id'])\n",
    "    \n",
    "    # ***Memory usage***\n",
    "    # Cast lats and lons as float16\n",
    "    unique_rideables['rideable_id'] = unique_rideables['rideable_id'].astype('int8')\n",
    "    \n",
    "    return unique_rideables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33e4cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.257417Z",
     "start_time": "2023-12-05T23:03:22.224328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rideable_id</th>\n",
       "      <th>rideable_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>classic_bike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>electric_bike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rideable_id  rideable_type\n",
       "0            0   classic_bike\n",
       "1            1  electric_bike"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_unique_rideables(data_sample['rideable_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0533def1",
   "metadata": {},
   "source": [
    "We can copy much the same code to do memberships as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d20b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.278367Z",
     "start_time": "2023-12-05T23:03:22.268089Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_unique_memberships(memberships):\n",
    "    \"\"\"\n",
    "    Identifies unique membership types, e.g. member vs casual. \n",
    "    \n",
    "    Args:\n",
    "        memberships: Series containing the membership_type data.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe containing unique memberships with a newly generated identifier\n",
    "    \"\"\"    \n",
    "    # Get unique rideables\n",
    "    unique_memberships = pd.DataFrame(memberships.drop_duplicates(keep='first'))\n",
    "    \n",
    "    # Now we'll use reset index to add a unique identifier\n",
    "    # Reset twice as we dropped a bunch of dupes so the index is currently not sequential\n",
    "    # Resetting with drop then resetting without createts a sequential set of IDs\n",
    "    unique_memberships = unique_memberships.reset_index(drop=True).reset_index(names=['membership_id'])\n",
    "    \n",
    "    # Rename member_casual\n",
    "    unique_memberships = unique_memberships.rename(columns={'member_casual': 'membership_type'})\n",
    "    \n",
    "    # ***Memory usage***\n",
    "    # Cast lats and lons as float16\n",
    "    unique_memberships['membership_id'] = unique_memberships['membership_id'].astype('int8')\n",
    "    \n",
    "    return unique_memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "977afe66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.337434Z",
     "start_time": "2023-12-05T23:03:22.309902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>membership_id</th>\n",
       "      <th>membership_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   membership_id membership_type\n",
       "0              0          member\n",
       "1              1          casual"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_unique_memberships(data_sample['member_casual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a0cdc",
   "metadata": {},
   "source": [
    "Now we'll address creating the ride time column from the started_at and ended_at columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cfde957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.356436Z",
     "start_time": "2023-12-05T23:03:22.345165Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_trip_duration_col(df):\n",
    "    \"\"\"\n",
    "    Calculates a series of trip durations in seconds, then downcasts data before returning. Memory conserved by casting \n",
    "      trip duration as int16. \n",
    "    \n",
    "    Args:\n",
    "        df: started_at and ended_at columns from dataframe. \n",
    "        \n",
    "    Returns:\n",
    "        Pandas series of trip durations.\n",
    "    \"\"\"\n",
    "    trip_duration = (pd.to_datetime(df['ended_at']) - pd.to_datetime(df['started_at'])).dt.seconds\n",
    "    \n",
    "    # ***Memory Usage***\n",
    "    # We choose int16 as it allows precision up to 32767, meaning in practicality, up to 8 hours of ride time\n",
    "    # This is a reasonable ride time (accuracy) cutoff, as only up to 45 minutes are included in the \"base price\"\n",
    "    trip_duration = trip_duration.astype('int16')\n",
    "    \n",
    "    return trip_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63692be8",
   "metadata": {},
   "source": [
    "We've completed our data formatting and ID generation processes, so now we need to actually build out the dataset. We'll be pulling multiple months of data, applying the formatting functions to each month as it's pulled. \n",
    "\n",
    "Below are some helper functions to reduce redundancy and make our final data build functions cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f65602fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.374034Z",
     "start_time": "2023-12-05T23:03:22.362859Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_on_columns(df, ref_df, left_on, right_on, new_col_name):\n",
    "    \"\"\"\n",
    "    Helper function to merge df with ref_df on specified columns and rename the new column.\n",
    "\n",
    "    Args:\n",
    "        df: The main DataFrame.\n",
    "        ref_df: The reference DataFrame to merge.\n",
    "        left_on: The column name in the main DataFrame to merge on.\n",
    "        right_on: The column name in the reference DataFrame to merge on.\n",
    "        new_col_name: The new column name after merging.\n",
    "\n",
    "    Returns:\n",
    "        Updated DataFrame after merge and column renaming.\n",
    "    \"\"\"\n",
    "    return (df.merge(ref_df[['station_id', 'int_station_id']], left_on=left_on, right_on=right_on, how='left')\n",
    "              .drop([right_on, left_on], axis=1)\n",
    "              .rename(columns={'int_station_id': new_col_name}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73616f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.394458Z",
     "start_time": "2023-12-05T23:03:22.385903Z"
    }
   },
   "outputs": [],
   "source": [
    "def concatenate_dataframes(dfs, drop_col=None):\n",
    "    \"\"\"\n",
    "    Concatenates a list of dataframes, drops duplicates, and resets the index.\n",
    "\n",
    "    Args:\n",
    "        dfs: List of dataframes to concatenate.\n",
    "        drop_col: Specific column to drop duplicates by, if needed.\n",
    "\n",
    "    Returns:\n",
    "        Concatenated dataframe.\n",
    "    \"\"\"\n",
    "    if drop_col is None:\n",
    "        return pd.concat(dfs).drop_duplicates().reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.concat(dfs).drop_duplicates(drop_col).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3a212f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.420957Z",
     "start_time": "2023-12-05T23:03:22.406092Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_and_report_memory_usage(initial_memory_values, dataframes):\n",
    "    \"\"\"\n",
    "    Calculates and reports the memory usage of the data processing.\n",
    "\n",
    "    Args:\n",
    "        initial_memory_values: List of memory usage values for each month's raw data.\n",
    "        dataframes: List of dataframes to include in the final memory usage calculation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Calculate initial and ending memory usage in MB\n",
    "    initial_memory_usage = sum(initial_memory_values) / 1024 / 1000\n",
    "    total_ending_memory_usage = sum(df.memory_usage(index=True, deep=True).sum() for df in dataframes) / 1024 / 1000\n",
    "    memory_saved = initial_memory_usage - total_ending_memory_usage\n",
    "\n",
    "    # Print memory usage information\n",
    "    print(f'Initial memory usage: {round(initial_memory_usage, 2)} MB')\n",
    "    print(f'Ending memory usage: {round(total_ending_memory_usage, 2)} MB')\n",
    "    print(f'Memory saved: {round(memory_saved, 2)} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83537f",
   "metadata": {},
   "source": [
    "Now we'll create a function that downloads a month of data and applies all the formatting work to that month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4a26600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.443952Z",
     "start_time": "2023-12-05T23:03:22.428888Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_formatted_month_rides(yyyymm):\n",
    "    \"\"\"\n",
    "    Downloads trip data file and conducts formatting. Formatting consists of creating a trip duration column.\n",
    "      \n",
    "    Args:\n",
    "        yyyymm: Year and month of target file.\n",
    "        \n",
    "    Returns:\n",
    "        Formatted dataframe of ride data, with multiple memory conservation steps applied. \n",
    "    \"\"\"\n",
    "    # Pull raw data\n",
    "    month_data = download_tripdata_file(yyyymm)\n",
    "    initial_memory_usage = month_data.memory_usage(index=True, deep=True).sum()\n",
    "\n",
    "    # Get unique stations, rideables, and memberships\n",
    "    unique_month_stations = identify_unique_stations(month_data)\n",
    "    unique_rideables = identify_unique_rideables(month_data['rideable_type'])\n",
    "    unique_memberships = identify_unique_memberships(month_data['member_casual'])\n",
    "\n",
    "    # Merge and format data\n",
    "    month_data = merge_on_columns(month_data, unique_month_stations, 'start_station_id', 'station_id', 'start_station_id')\n",
    "    month_data = merge_on_columns(month_data, unique_month_stations, 'end_station_id', 'station_id', 'end_station_id')\n",
    "    month_data = month_data.merge(unique_rideables, on='rideable_type', how='left')\n",
    "    month_data = month_data.merge(unique_memberships, left_on='member_casual', right_on='membership_type', how='left')\n",
    "\n",
    "    # Add trip duration\n",
    "    month_data['trip_duration'] = create_trip_duration_col(month_data[['started_at', 'ended_at']])\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = [\n",
    "        'ended_at', \n",
    "        'rideable_type', \n",
    "        'member_casual',\n",
    "        'membership_type',\n",
    "        'start_lat', \n",
    "        'end_lat', \n",
    "        'start_lng', \n",
    "        'end_lng', \n",
    "        'start_station_name', \n",
    "        'end_station_name'\n",
    "    ]\n",
    "    month_data = month_data.drop(drop_cols, axis=1)\n",
    "    \n",
    "    return month_data, unique_month_stations, unique_rideables, unique_memberships, initial_memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b0736",
   "metadata": {},
   "source": [
    "Finally, we use multiple iterations of create_formatted_month_rides to build a multi-month dataset with dramatically reduced memory consumption vs the raw files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535f95b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:05:40.365540Z",
     "start_time": "2023-12-05T23:05:40.343356Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_month_data(month, data_directory):\n",
    "    \"\"\"\n",
    "    Process and save data for a single month to a parquet file.\n",
    "\n",
    "    Args:\n",
    "        month: The month to process in yyyymm format.\n",
    "        data_directory: The directory where the parquet file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - The path to the saved parquet file.\n",
    "            - Unique stations DataFrame.\n",
    "            - Unique rideables DataFrame.\n",
    "            - Unique memberships DataFrame.\n",
    "            - Memory usage of the processed month data.\n",
    "    \"\"\"\n",
    "    month_data, unique_stations, unique_rideables, unique_memberships, memory_usage = create_formatted_month_rides(month)\n",
    "    file_path = f'{data_directory}/{month}_data.parquet'\n",
    "    month_data.to_parquet(file_path)\n",
    "\n",
    "    return file_path, unique_stations, unique_rideables, unique_memberships, memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d05189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.642314Z",
     "start_time": "2023-12-05T23:03:22.609403Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_data(months_to_pull):\n",
    "    \"\"\"\n",
    "    Pulls requested trip data for each month in parallel, writes it to \n",
    "    parquet files in a parallel 'data' directory, and disposes of it from \n",
    "    memory. Memory conserved by dropping Citi Bike unique ID and by not \n",
    "    storing month data in memory. Unique data for stations, rideables, \n",
    "    and memberships are still aggregated in memory. The unique data \n",
    "    tables function as dimension tables are are written to a data \n",
    "    directory as such. \n",
    "\n",
    "    Args:\n",
    "        months_to_pull: List of yyyymm format months to create data with.\n",
    "\n",
    "    Returns: \n",
    "        A tuple containing:\n",
    "            - A list of file paths to the parquet files containing the ride data.\n",
    "            - DataFrames of all unique stations, rideables, and memberships.\n",
    "    \"\"\"\n",
    "    data_directory = '../data'  # Adjust this path according to your directory structure\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=len(months_to_pull)) as executor:\n",
    "        results = executor.map(process_month_data, months_to_pull, [data_directory] * len(months_to_pull))\n",
    "\n",
    "    # Unpack results\n",
    "    data_file_paths, all_unique_stations, all_unique_rideables, all_unique_memberships, month_file_memory_values = zip(*results)\n",
    "\n",
    "    # Concatenate and process unique dataframes\n",
    "    all_unique_stations = concatenate_dataframes(all_unique_stations, 'station_name')\n",
    "    all_unique_rideables = concatenate_dataframes(all_unique_rideables, 'rideable_type')\n",
    "    all_unique_memberships = concatenate_dataframes(all_unique_memberships, 'membership_type')\n",
    "\n",
    "    # Write uniques tables to data directory for use as fact tables\n",
    "    for df, filename in zip([all_unique_stations, all_unique_rideables, all_unique_memberships], ['stations', 'rideable_types', 'membership_types']):\n",
    "        filepath_no_type = os.path.join(data_directory, filename)\n",
    "        df.to_parquet(f'{filepath_no_type}.parquet')\n",
    "\n",
    "    # Calculate and report memory usage\n",
    "    calculate_and_report_memory_usage(\n",
    "        month_file_memory_values, \n",
    "        [all_unique_stations, all_unique_rideables, all_unique_memberships]\n",
    "    )\n",
    "\n",
    "    return data_file_paths, all_unique_stations, all_unique_rideables, all_unique_memberships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bbfda",
   "metadata": {},
   "source": [
    "## Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60234358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable is at the top of the notebook also, but including here for convienience\n",
    "FETCH_RAW_DATA = True\n",
    "\n",
    "# SET MONTHS TO PULL\n",
    "months_to_pull = ['202404', '202405', '202406']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0673e67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:05:40.314640Z",
     "start_time": "2023-12-05T23:03:22.653496Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe4 in position 99: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FETCH_RAW_DATA \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     ride_data_paths, unique_stations, unique_rideables, unique_memberships \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonths_to_pull\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36mget_all_data\u001b[0;34m(months_to_pull)\u001b[0m\n\u001b[1;32m     24\u001b[0m     results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(process_month_data, months_to_pull, [data_directory] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(months_to_pull))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Unpack results\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m data_file_paths, all_unique_stations, all_unique_rideables, all_unique_memberships, month_file_memory_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Concatenate and process unique dataframes\u001b[39;00m\n\u001b[1;32m     30\u001b[0m all_unique_stations \u001b[38;5;241m=\u001b[39m concatenate_dataframes(all_unique_stations, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m, in \u001b[0;36mprocess_month_data\u001b[0;34m(month, data_directory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_month_data\u001b[39m(month, data_directory):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Process and save data for a single month to a parquet file.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m            - Memory usage of the processed month data.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     month_data, unique_stations, unique_rideables, unique_memberships, memory_usage \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_formatted_month_rides\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_data.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m     month_data\u001b[38;5;241m.\u001b[39mto_parquet(file_path)\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m, in \u001b[0;36mcreate_formatted_month_rides\u001b[0;34m(yyyymm)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mDownloads trip data file and conducts formatting. Formatting consists of creating a trip duration column.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m  \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Formatted dataframe of ride data, with multiple memory conservation steps applied. \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Pull raw data\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m month_data \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_tripdata_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43myyyymm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m initial_memory_usage \u001b[38;5;241m=\u001b[39m month_data\u001b[38;5;241m.\u001b[39mmemory_usage(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get unique stations, rideables, and memberships\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m, in \u001b[0;36mdownload_tripdata_file\u001b[0;34m(yyyymm, sample_only)\u001b[0m\n\u001b[1;32m     34\u001b[0m csv_datasets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[0;32m---> 36\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     csv_datasets\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     41\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(csv_datasets)\n",
      "File \u001b[0;32m~/Documents/Data Science Portfolio/citibike/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Data Science Portfolio/citibike/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Documents/Data Science Portfolio/citibike/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Data Science Portfolio/citibike/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Data Science Portfolio/citibike/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe4 in position 99: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "if FETCH_RAW_DATA is True:\n",
    "    ride_data_paths, unique_stations, unique_rideables, unique_memberships = get_all_data(\n",
    "        months_to_pull\n",
    "    )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a98cf",
   "metadata": {},
   "source": [
    "Let's also output our unique lookup tables for reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c407a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.499080Z",
     "start_time": "2023-12-05T23:03:22.473531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_station_id</th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6230.04</td>\n",
       "      <td>FDR Drive &amp; E 35 St</td>\n",
       "      <td>40.743954</td>\n",
       "      <td>-73.971390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5382.07</td>\n",
       "      <td>Forsyth St &amp; Grand St</td>\n",
       "      <td>40.717739</td>\n",
       "      <td>-73.993385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5971.08</td>\n",
       "      <td>E 20 St &amp; 2 Ave</td>\n",
       "      <td>40.735790</td>\n",
       "      <td>-73.981689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_station_id station_id           station_name        lat        lng\n",
       "0               0    6230.04    FDR Drive & E 35 St  40.743954 -73.971390\n",
       "1               1    5382.07  Forsyth St & Grand St  40.717739 -73.993385\n",
       "2               2    5971.08        E 20 St & 2 Ave  40.735790 -73.981689"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_stations.head(3) # sample as this has a few hundred stations in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfec875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.529030Z",
     "start_time": "2023-12-05T23:03:22.508834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rideable_id</th>\n",
       "      <th>rideable_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>electric_bike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>classic_bike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rideable_id  rideable_type\n",
       "0            0  electric_bike\n",
       "1            1   classic_bike"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_rideables.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b7e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:03:22.561180Z",
     "start_time": "2023-12-05T23:03:22.538879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>membership_id</th>\n",
       "      <th>membership_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   membership_id membership_type\n",
       "0              0          member\n",
       "1              1          casual"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_memberships.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b37a2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2e0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:27:19.590143Z",
     "start_time": "2023-12-05T23:27:09.189613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rides = pd.DataFrame()\n",
    "for path in ride_data_paths:\n",
    "    rides = pd.concat([rides, pd.read_parquet(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb932ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T23:27:46.460817Z",
     "start_time": "2023-12-05T23:27:46.450400Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5000639 entries, 0 to 783575\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Dtype \n",
      "---  ------            ----- \n",
      " 0   ride_id           object\n",
      " 1   started_at        object\n",
      " 2   start_station_id  int16 \n",
      " 3   end_station_id    int16 \n",
      " 4   rideable_id       int8  \n",
      " 5   membership_id     int8  \n",
      " 6   trip_duration     int16 \n",
      "dtypes: int16(3), int8(2), object(2)\n",
      "memory usage: 152.6+ MB\n"
     ]
    }
   ],
   "source": [
    "rides.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabe01c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citibike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
